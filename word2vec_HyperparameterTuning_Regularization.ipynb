{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POPULAR METHOD TO GENERATE WORD EMBEDDINGS\n",
    "# WORD2VEC MAPS WORDS TO A VECTOR SPACE OF GIVEN DIMENSION\n",
    "# CALCULATIONS CAN BE PERFOMED TO FIND RELATIONSHIP BETWEEN THEM\n",
    "# CBOW - CONTINUOUS BAG OF WORDS ( TARGET WORD PREDICTED FROM CONTEXT)\n",
    "#        TAKES A SENTENCE, AND USES A WINDOW SIZE FOR ITERATING OVER THE SENTENCE\n",
    "#        THEN WE TRY TO PREDIICT CENTRE WORD FROM THE CONTEXT WORDS\n",
    "#        WE HAVE TO CHOOSE SIZE OF EACH VECTOR AS WELL\n",
    "#        HIDDEN LAYER IS ALSO OF SAME SIZE\n",
    "#        WE PASS THE ONE HOT VECTOR OF THE CONTEXT WORDS AND TRY TO PREDICT THE CENTRE WORD\n",
    "#        THIS IS DONE USING NEURAL NETWORK AND ERROR BACKPROPAGATION\n",
    "#        THE CONTEXT WORDS VECTORS SHARE THE PREDICTED WORD VECTOR AND HIDDEN LAYER\n",
    "#        THE PROBABILITIES OF THE PREDICTED WORD ARE COMPARED WITH THE VECTOR OF THE TARGET WORD\n",
    "#        THEN THE WEIGHTS ARE UPDATED.THE WINDOW SLIDES AND THE PROCESS CONTINUES\n",
    "#        ONCE DONE.THESE UPDATED WEIGHTS BECOME OUR SET OF VECTORS\n",
    "# SKIP GRAM - (CONTEXT WORDS PREDICED FROM TARGET WORD)\n",
    "#            WE CHOOSE WINDOW SIZE AND GIVE CENTRE WORD AS INPUT\n",
    "#            TRY TO PREDICT CONTEXT WORDS AND UPDATE WEIGHTS\n",
    "#            HIDDEN LAYER SHARED BY THE CONTEXT WORDS\n",
    "#            THEN WE TAKE THE INPUT WEIGHT MATRIX AND MULTIPLY IT WITH THE ONE HOT VECTOR TO OBTAIN ITS WORD VECTOR\n",
    "# USUALLY WINDOW SIZE 5-10 VECTOR SIZE AROUND 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits=load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D,MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Dropout,ReLU,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(3,3),padding='valid',input_shape=(8,8,1),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(2,2),activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=digits.images\n",
    "df_=df.reshape(len(df),8,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x,y_train,y=train_test_split(df_,digits.target,random_state=42,test_size=.3,stratify=digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validate,x_test,y_validate,y_test=train_test_split(x,y,random_state=42,test_size=.5,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1257 samples, validate on 270 samples\n",
      "Epoch 1/10\n",
      "1257/1257 [==============================] - 4s 3ms/step - loss: 1.8580 - acc: 0.4519 - val_loss: 1.2758 - val_acc: 0.6704\n",
      "Epoch 2/10\n",
      "1257/1257 [==============================] - 1s 541us/step - loss: 0.7842 - acc: 0.8274 - val_loss: 0.5112 - val_acc: 0.8741\n",
      "Epoch 3/10\n",
      "1257/1257 [==============================] - 1s 414us/step - loss: 0.3264 - acc: 0.9260 - val_loss: 0.3185 - val_acc: 0.8889\n",
      "Epoch 4/10\n",
      "1257/1257 [==============================] - 0s 379us/step - loss: 0.2049 - acc: 0.9475 - val_loss: 0.2032 - val_acc: 0.9444\n",
      "Epoch 5/10\n",
      "1257/1257 [==============================] - 0s 356us/step - loss: 0.1166 - acc: 0.9745 - val_loss: 0.1625 - val_acc: 0.9519\n",
      "Epoch 6/10\n",
      "1257/1257 [==============================] - 0s 249us/step - loss: 0.0838 - acc: 0.9833 - val_loss: 0.1636 - val_acc: 0.9444\n",
      "Epoch 7/10\n",
      "1257/1257 [==============================] - 0s 249us/step - loss: 0.0663 - acc: 0.9889 - val_loss: 0.1560 - val_acc: 0.9593\n",
      "Epoch 8/10\n",
      "1257/1257 [==============================] - 0s 258us/step - loss: 0.0520 - acc: 0.9912 - val_loss: 0.1322 - val_acc: 0.9593\n",
      "Epoch 9/10\n",
      "1257/1257 [==============================] - 0s 246us/step - loss: 0.0378 - acc: 0.9952 - val_loss: 0.1157 - val_acc: 0.9593\n",
      "Epoch 10/10\n",
      "1257/1257 [==============================] - 0s 247us/step - loss: 0.0286 - acc: 0.9992 - val_loss: 0.1135 - val_acc: 0.9704\n"
     ]
    }
   ],
   "source": [
    "y=model.fit(x_train,y_train,epochs=10,validation_data=(x_validate,y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 267us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08194828938554835, 0.9777777777777777]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 0s 330us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02606237841460353, 0.9976133651551312]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is a process to reduce overfitting. It is a\n",
    "# mathematical way of inducing a warning into the model’s learning process\n",
    "# when it accommodates noise. To give a more realistic definition, it is a\n",
    "# method to penalize the model weights in the event of overfitting.\n",
    "# The process of regularization adds the weights of the edges to\n",
    "# the defined loss function and holistically represents a higher loss. The\n",
    "# network then tunes itself to reduce the loss and thereby makes the weight\n",
    "# updates in the right direction; this works by ignoring the noise rather than\n",
    "# accommodating it in the learning process.\n",
    "# The process of regularization can be demonstrated as\n",
    "# Cost Function = Loss (as defined for the model) + Hyperparameter × [Weights]\n",
    "# L1 Regularization\n",
    "# the absolute weights are added to the loss function. To\n",
    "# make the model more generalized, the values of the weights are reduced\n",
    "# to 0, and therefore this method is strongly preferred when we are trying to\n",
    "# compress the model for faster computation.\n",
    "# L2 Regularization\n",
    "# In L2 regularization, the squared weights are added to the loss function. To\n",
    "# make the model more generalized, the values of the weights are reduced to\n",
    "# near 0 (but not actually 0), and hence this is also called the “weight decay”\n",
    "# method. In most cases, L2 is highly recommended over L1 for reducing\n",
    "# overfitting\n",
    "# model.add(Dense(256, input_dim=128,\n",
    "# kernel_regularizer=regularizers.l2(0.01)) where 0.01 is the hyperparameter lambda\n",
    "# Dropout Regularization\n",
    "# the model arbitrarily drops or deactivates a few neurons for a layer during each iteration.\n",
    "# process repeats for each iteration with randomness\n",
    "# efficient due to the reduced computation and works intuitively in reducing the overfitting\n",
    "# Dropout(rate, noise_shape=None, seed=None)\n",
    "# model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER TUNING\n",
    "# i) NUMBER OF INPUT NODES\n",
    "# simple rule of thumb for selecting the number of neurons in the first layer is to refer to the number of input dimensions.\n",
    "# If the final number of input dimensions in a given training dataset (this includes the one-hot encoded features also) is x,\n",
    "# we should use at least the closest number to 2x in the power of 2.\n",
    "# ii)NUMBER OF LAYERS\n",
    "# The problem is that with an increased number of layers, the training time and computation increase significantly.\n",
    "# You would need a higher number of epochs to see promising results\n",
    "# For the last hidden layer (not the output layer), try keeping the number of neurons to at least around 30–40% \n",
    "# of the input size.\n",
    "# In case of large network use tapering size architecture (eg:1st 8-512; 2nd 8-256 ; 3rd 8-128 and so on...)\n",
    "# For wider networks always use L2 Regularization\n",
    "# iii)WEIGHT INITIALIZATION\n",
    "# By default, the Keras framework uses glorot uniform initialization, also called Xavier uniform initialization.\n",
    "# Other popular options to select are ‘He Normal’ and ‘He Uniform’ initialization and ‘lecun normal’ and ‘lecun uniform’\n",
    "# initialization.\n",
    "# model.add(Dense(64,activation=\"relu\", input_dim = 32, kernel_initializer = \"random_uniform\",bias_initializer = \"zeros\"))\n",
    "# iv)BATCH SIZE\n",
    "# usually go for 32 or 64 as they give a smooth learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram works well with small amount of data and is found to represent rare words well\n",
    "# CBOW is faster and has better representations for more frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model which uses L1 regularization technique is called Lasso regression\n",
    "# model which uses L2 is called Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
